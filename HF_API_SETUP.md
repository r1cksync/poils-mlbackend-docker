# ğŸš€ Getting Started with Hugging Face API

## âœ… Changes Made

The backend has been **completely refactored** to use **Hugging Face Inference API** instead of loading models locally. This eliminates all memory issues and makes deployment instant!

### What Changed:
- âŒ Removed: PyTorch, Transformers, OpenCV, NumPy (heavy dependencies)
- âœ… Added: Lightweight API client using `httpx`
- âœ… Result: **~50MB total size** (was 2GB+)
- âœ… Deployment: Works on **Vercel Free tier** and **Render Free tier**
- âœ… Startup: **Instant** (no model loading wait)

---

## ğŸ”‘ Get Your Hugging Face API Key (Optional but Recommended)

### Free Tier (No API Key)
- âœ… Works immediately
- âš ï¸ Rate limited (few requests per minute)
- âš ï¸ May have cold start delays

### With API Key (Recommended)
- âœ… Higher rate limits
- âœ… Faster response times
- âœ… Priority processing

### Steps to Get API Key:

1. **Go to Hugging Face**: https://huggingface.co/

2. **Sign Up** (if you don't have an account):
   - Click "Sign Up" (top right)
   - Create free account

3. **Generate API Token**:
   - Go to: https://huggingface.co/settings/tokens
   - Click "New token"
   - Name: `hindi-ocr-api`
   - Type: "Read" (default)
   - Click "Generate"
   - **Copy the token** (starts with `hf_...`)

4. **Add to Render Environment Variables**:
   - Go to Render Dashboard
   - Select your service
   - Go to "Environment" tab
   - Add new variable:
     - Key: `HUGGINGFACE_API_KEY`
     - Value: `hf_xxxxxxxxxxxxxxxxxxxxx` (paste your token)
   - Click "Save Changes"

5. **Redeploy** (if needed):
   - Service will auto-restart with new env var

---

## ğŸ“Š How It Works Now

```
User Uploads Image
    â†“
Your FastAPI Backend
    â†“
Hugging Face Inference API
(microsoft/trocr-base-printed model)
    â†“
Returns Extracted Text
    â†“
Your API Returns to User
```

### Advantages:
- âœ… **No memory issues** - model runs on HF servers
- âœ… **Instant startup** - no model loading
- âœ… **Auto-scaling** - HF handles the load
- âœ… **Always updated** - latest model version
- âœ… **Free tier** - works without payment

### Performance:
- **First request**: May take 20-30 seconds (model cold start on HF)
- **Subsequent requests**: 2-5 seconds
- **With API key**: Faster and more reliable

---

## ğŸš€ Deploy Instructions

### Option 1: Render (Recommended)

1. **Push to GitHub** (already done)

2. **Create Web Service on Render**:
   - Go to https://render.com
   - Click "New +" â†’ "Web Service"
   - Connect your repository: `poils-mlbackend-docker`
   - Configure:
     - **Name**: `hindi-ocr-api`
     - **Environment**: Docker
     - **Branch**: main
     - **Instance Type**: Free (works now!)

3. **Add Environment Variable** (optional):
   - `HUGGINGFACE_API_KEY`: (your token)

4. **Deploy** - Should complete in 2-3 minutes!

### Option 2: Vercel

```powershell
# Navigate to directory
cd "backend/docker-python backend"

# Deploy to Vercel
vercel --prod
```

Add environment variable in Vercel dashboard:
- `HUGGINGFACE_API_KEY`: (your token)

---

## ğŸ§ª Test Locally

```powershell
# Navigate to directory
cd "backend/docker-python backend"

# Install dependencies (very fast now!)
pip install -r requirements.txt

# Set API key (optional)
$env:HUGGINGFACE_API_KEY="hf_your_token_here"

# Run server
uvicorn main:app --reload

# Test
curl http://localhost:8000/health
```

Visit http://localhost:8000/docs to test the API

---

## ğŸ“ API Usage

### Upload Image

```powershell
curl -X POST "https://your-api.onrender.com/api/ocr/extract" `
  -F "image=@hindi_text.jpg"
```

### Response

```json
{
  "success": true,
  "text": "à¤¨à¤¿à¤•à¤¾à¤²à¤¾ à¤—à¤¯à¤¾ à¤¹à¤¿à¤‚à¤¦à¥€ à¤ªà¤¾à¤ ",
  "confidence": 0.85,
  "processing_time": 2.5,
  "image_info": {
    "width": 800,
    "height": 600,
    "format": "JPEG"
  },
  "device": "cloud-api"
}
```

---

## ğŸ”„ Integration with Next.js

Update `backend/nextjs-backend/.env.local`:

```env
# Your deployed Python backend URL
PYTHON_BACKEND_URL=https://your-service.onrender.com
```

Then use as before - no code changes needed!

---

## ğŸ¯ Benefits Summary

| Feature | Before (Local Model) | After (HF API) |
|---------|---------------------|----------------|
| **Size** | 2GB+ | ~50MB |
| **Memory** | 2GB+ RAM | 512MB RAM |
| **Startup** | 30-60 seconds | Instant |
| **Deployment** | Render Starter+ | Free tier âœ… |
| **Maintenance** | Manual updates | Auto-updated |
| **Scaling** | Limited | Unlimited |
| **Cost** | $7+/month | $0 (free tier) |

---

## ğŸ†˜ Troubleshooting

### Issue: "Model is loading" error

**Cause**: HF model has cold start (first request)

**Solution**: Wait 20-30 seconds and retry. After first request, it's fast.

### Issue: Rate limit errors

**Cause**: Using free tier without API key

**Solution**: Add `HUGGINGFACE_API_KEY` environment variable

### Issue: Slow responses

**Cause**: Free tier has lower priority

**Solution**: 
- Add API key (free)
- Model stays warm after recent use

---

## ğŸ‰ You're Done!

Your API is now:
- âœ… Lightweight (~50MB)
- âœ… Fast to deploy
- âœ… Runs on free tier
- âœ… No memory issues
- âœ… Auto-scaling

Deploy to Render and start using your Hindi OCR API!

**Next Steps:**
1. Deploy to Render
2. Get HF API key (optional)
3. Test with your Hindi documents
4. Integrate with Next.js frontend
